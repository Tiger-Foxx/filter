bon il serait temps que je parle du projet de recherche que j'ai à faire bon en fait comme je l'avais déjà expliqué mon projet de recherche va porter sur l'optimisation du filtrage au réseau notamment ce qui concerne les systèmes de prévention d'inclusion ou encore les parfaits web tout ce qui fait du filtrage au réseau voilà un peu en quoi ça va consister et quand je dis l'optimisation des filtres réseau ça va surtout être une optimisation au niveau des règles au niveau de l'architecturé même de comment est ce que tu peux les deux sont placés au niveau de d'à peu près tout en fait bon au niveau des règles sud et de cette histoire de commencer à placer tout donc le projet va surtout le projet de recherche je vais lui dire projet de recherche Je pr en fait que c un projet de recherche parce que le livrable qui est vraiment attendu l est surtout une id et une preuve de concept Donc c pas un produit final livrable et tout et tout et tout avec s toutes les r prises en compte etc etc Non pas vraiment Le produit livrable est vraiment juste une preuve de concept testée. Un petit truc, un petit quelque chose qui ne prend même peut-être pas tous les types de règles en compte, mais vraiment un tout petit quelque chose écrit vite fait en C++ ou en C qui permet vraiment de faire le filtrage en question. Je vais t'expliquer les tenants et les aboutissants de ce module-là qu'on cherche à développer en fait. Ce module qu'on cherche à développer comme preuve de concept pour le papier qu'on va écrire, ou du moins pour le papier qu'on va essayer d'écrire. Donc c'est ça, il ne faut pas partir dans l'idée que vraiment c'est un produit final et tout et tout, non. C'est vraiment juste une preuve de concept. Tout ce qui est bling-bling et tout, tout ce qui est design et tout, tout ce qui est machin, non, ça ne nous intéresse pas. Nous, on veut juste de la performance et vraiment de la preuve de concept. Maintenant, je vais t'expliquer en quoi consiste notre module.
Bon, pour cela, pour faire les choses bien, je vais commencer par te présenter le concept de contexte. On va parler du contexte. Bon, à l'heure d'aujourd'hui, de plus en plus d'entreprises ont besoin de mettre des filtreurs réseau dans leurs infrastructures. Donc, certaines entreprises se retrouvent avec des pare-feu, avec des systèmes de prévention d'intrusion. Ils peuvent aligner même plusieurs systèmes de prévention d'intrusion. D'ailleurs, je note que c'est même dans ce cas-là que notre solution est même encore la plus optimale. Ils peuvent avoir des parfaits web, des parfaits web placés comme des reverse proxy, par exemple, dont plusieurs filtreurs réseau empilés sur une chaîne de filtrage, en fait. Et après avoir fait des mesures, on a constaté que plus on ajoute de filtreurs réseau, plus on gagne en latence et on perd en débit. En d'autres termes, plus on ajoute de filtreur réseau, moins c'est correct en réalité. Moins la vitesse du réseau est élevée. Voilà. Le nombre de filtreurs réduit considérablement la vitesse des paquets dans le réseau. Ce qui est plutôt logique, mais on a quand même testé et on a mesuré cela. Ça peut diminuer jusqu'à 80% du débit. ce qui est très intéressant. Aussi on a constaté que parfois lorsqu'on a une chaîne de filtrage, un paquet qui arrive dans la cha va se retrouver rejet peut tout au bout de la cha alors que si on l rejet depuis le d il n pas eu besoin de traverser toute la chaîne parce que lorsque lorsque ce paquet traverse toute la chaîne de filtrage le paquet en question a fait un parcours en quelque sorte est inutile le paquet a fait un parcours complètement inutile qui ne servait absolument à rien c'est à dire que le passage du paquet sur les machines a fait dépenser de l'énergie de la ressource cpu bref tout ce qui implique le traitement d'un paquet du débit de la bande passante bref etc etc donc le passage du paquet ne servait à rien du coup ça aurait été vraiment l'idéal de bloquer le paquet de rejeter le paquet directement au tout début de la chaîne sans qu'il n'ait besoin de faire le parcours entier voilà un peu le contexte et ce qui serait intéressant en fait. Maintenant, je vais t'expliquer en fait ce à quoi on pense, c'est-à-dire je vais te dire quelle est la solution que l'on propose, ce qu'on va tester, ce qu'on va devoir implémenter pour tester, ce qu'on a fait dans un premier temps pour tester des hypothèses, ce qu'on va implémenter là tout de suite pour tester, de parler même des technos que l'on va utiliser, des analogies que l'on va faire, etc. Donc maintenant, on va beaucoup discuter. Donc tout de suite, je vais te présenter la partie maintenant sur ce que nous on pense faire pour ce papier.
du coup ce qu'on proposait donc et on proposait donc une solution qui ne nécessiterait pas que les entreprises d'aujourd'hui modifie le système de prévention d'intrusion ou modifie leur parafeu ou modifie leur firewall web donc on ne veut pas que les applications modifient que les grands fabricants d'aujourd'hui modifient leurs applications sauf si notre idée leur plaît et qu'ils veulent intégrer ça là dedans mais là c'est autre chose on veut faire un truc totalement indépendant de par exemple on sait par exemple ne veut pas que notre modifie son code non on ne veut pas ça bon ça c'était déjà pour le petit discrément sauf si évidemment notre déplacement il veut l'intégrer mais ça c'est déjà le petit discrément pour commencer bon maintenant commençons à expliquer ce qui se passe maintenant c'est c'est que nous voulons faire un module, un module en réalité, qui va se placer au tout d de la cha Le module en question se place au d de la cha de filtrage Et ce que le module va faire c qu va juste donner au module on va juste indiquer au module Là, je te parle du module dans son cas idéal, c'est-à-dire vraiment dans sa conception idéale. Donc, si, imaginons si le papier est vraiment reçu, la preuve de concept marche et que vraiment une équipe de dev le fabrique et tout, mais je suis en train de une preuve de concept. Je dis dans son cas idéal, on placerait le module au-dessus de la chaîne. On indiquerait juste au module où se trouvent les règles. Oui, on dirait au module où se trouvent les règles des différents filtres. Par exemple, les règles des IPS qui sont sur le réseau, les règles des firewall web. Bref, on lui indiquerait où sont les règles. Il irait chercher les règles en question, les fichiers de règles. Il irait les chercher. Il les cherche. il formule toutes ces il agrège en réalité il fait une agr de toutes ces r dans un seul ensemble de r un grand ensemble de r tous il agr et il filtre les paquets directement l au d Donc, les paquets qui devaient être rejetés n'ont plus besoin de passer par la chaîne en question. Ils sont directement rejetés au début. C'est vrai que là, maintenant, j'ai parlé d'agréger les paquets et les règles. En réalité, j'ai parlé d'agréger les règles. Et tu risques de trouver quelques petits problèmes à ça. Déjà, premièrement, il faudrait que ce soit rapide. Il faudrait que notre système soit rapide, que ce soit dans le matching des règles, que ce soit dans le filtrage, que ce soit dans le réassemblage des flux TCP. Ça doit aussi être rapide. il y a des outils open source comme Snort par exemple qui est un IPS qui fait du réassemblage de paquets TCP à une vitesse vraiment absurde. Donc on pourrait s de lui par exemple Ou s existe d des librairies qui peuvent faire extr vite ce serait bien Parce que par exemple pour rechercher un pattern à l'intérieur d'une requête, par exemple, qui a été envoyée, il faut bien reconstituer le flux TCP de la requête en question avant de faire le truc et laisser passer la requête ou la drop, par exemple. Donc, il y a le problème de réassemblage, ça doit être rapide. Il y a le matching des règles qui doit être rapide. Pour ça, Il y a des algorithmes utilisés par les IDS aujourd'hui, comme l'algorithme de Haro Horacic, ou encore Hyperscan, par exemple, qui est vraiment extrêmement rapide et dont on pourrait se servir, par exemple, le matching de règles, et d'autres algorithmes complexes et optimisés. On est en recherche, on est libre de le faire. Donc, je me disais quelque chose comme ça. Et maintenant, ça, c'est vraiment dans le cas idéal. Et maintenant, je vais te donner d'autres détails par rapport à ce fameux module, vraiment son comportement et tout.
de plus , et j'ai ouble de le dire , nous on a meme pas besoin pour le moment de savoir quelle est la rele qui a match et a fait drop le paquet , mais juste de drop le packet !!! ce qui est vraiment super car ca nous enleve une certaine restriction et etend d'avantage nos horizons de performances , de fusions de regles et de matching d'algorihmes ..etc !!
car on ne se pose plus en plus de bloquer , de savoir quel regles a ete a l'origine du blocage (notre module s'en fiche ,du moins , a ce stade de preuve de concept)
car on pourrait bibien presenter un module qui ne sait peut etre pas quelle reglew a match et ce serait son defaut peut etre mais avec pleins d'avantages comme la vitesse et de meilleurs algorithmes
Le comportement du module serait en fait même, comme je t'ai dit, il faut qu'il agrège les règles. Donc, l'idéal serait que les règles soient écrites dans un seul langage unifié. Donc, il prend, par exemple, parce que je pense que prendre des règles de pas à feu, par exemple, et les traduire en règles de GPS, moi je proposais qu'on traduise toutes les règles qu'on a en un système de langage qui ressemble à celui des systèmes de prévention d'intrusion parce que les règles des systèmes de prévention d'intrusion peuvent étonnamment correspondre aux règles d'à peu près tout donc les règles de firewall et les règles de web firewall peuvent être traduites en règles d'IPS même si on n'est pas obligé de prendre exactement l'écriture comme avec les IPS. On peut aussi reformuler le tout. Même les règles des IPS, on les arrange aussi dans notre langage à nous. Donc, on prend les règles d'IPS, on les arrange à nous dans notre langage. On pourrait, mais si on voit que c'est mieux de garder le langage des IPS comme notre DSL et qu ram juste les r des autres dans ce langage ce serait tr int Donc il y a cette probl d de r de r des r dans un certain format Même si, en réalité, ce n'est pas vraiment un problème parce que pour notre preuve de concept, on peut juste prendre juste d'abord des règles d'IPS. Donc, on gère juste des IPS, sachant que le travail de prendre les règles de PAF ou des règles de WebFirewallet traduites en règles d'IPS, c'est pas vraiment ça le travail de recherche c'est pas vraiment le coeur de notre truc donc on n'a pas vraiment besoin de pour notre preuve de concept on n'a pas vraiment besoin d'implémenter cette traduction de règles parce que la traduction là c'est vraiment c'est vraiment un détail à ce niveau là qui va nous intéresser on va on va en parler on va en parler beaucoup plus tard je disais donc ça donc il faut agréger les règles en question une fois qu'on les a agrégés maintenant il faut procéder au filtrage Maintenant qu'on les a agrégés, il y a quelque chose qui se passe. C'est que tu me diras, on aura maintenant un filtreur au début de la chaîne, mais filtreur qui aura les règles de tout le monde. Si on ne fait que additionner les règles les unes aux autres de façon séquentielle, ce ne sera pas trop diff fera juste comme si les filtreurs qu avait auparavant dans la cha se retrouve maintenant sur un seul ordinateur peut que ce sera plus rapide peut pas mais je ne sais pas par contre il ya moyen d'optimiser cela raison pour laquelle dans un premier temps nos expériences consistait à voir à implémenter un filtre séquentiel c'est à dire qu'on avait un filtreur qui prenait un ensemble de règles données, un autre ensemble de règles données et un autre ensemble de règles données. Quand un paquet arrivait, il testait avec le premier ensemble de règles, puis il testait avec le second, puis il testait avec le troisième. On a mesuré la vitesse, c'était assez lent. Puis on a fait une méthode de filtrage parallèle. Lorsqu'un paquet arrivait le premier l'est le processus de filtrage du premier ensemble une procédure de filtrage du deuxième ensemble le processus de filtrage du troisième ensemble de règles s'exécutait simultanément les processus les filtres il y avait en réalité trois walkers ou trois threads qui tournait sur des queues différents c'était vraiment du vrai multi-threading du vrai parall les processus de filtrer s simultan et on faisait un et logique entre les trois cette r entre les trois filtreurs pour voir si le paquet passe effectivement ou s est drop C assez rapide voire m très rapide, en tout cas plus rapide que le filtrage séquentiel. Mais le problème, c'est qu'en termes de consommation énergétique et en termes de consommation CPU, c'était désastreux, c'était une catastrophe. Ça prenait beaucoup trop de ressources. C'est là maintenant qu'on va parler de la façon dont on va implémenter notre preuve de concept ici, qui est la troisième façon et qui est vraiment le cœur du travail. Cette troisième façon-là, elle est très difficile. Il faudra utiliser des technologies poussées, des algorithmes vraiment avancés, s'inspirer vraiment des plus grands comme les SNOT et les Suricata, etc. etc. Rechercher des papiers, faire vraiment des algos vraiment optimisés, que ce soit pour le réassemblage et tout. Cette troisième méthode-là, qui est différente du filtrage séquentiel et du filtrage parallèle, je vais te la décrire tout de suite en quoi est-ce qu'elle va consister. C'est un compromis entre les deux et c'est elle qu'on veut tester. On va comprendre quand on aura fini d'implémenter ça vraiment correctement. On va faire quelques tests dont je vais te parler aussi. Donc pour l'instant, je vais te décrire maintenant la méthode en question.
de plus , et j'ai ouble de le dire , nous on a meme pas besoin pour le moment de savoir quelle est la rele qui a match et a fait drop le paquet , mais juste de drop le packet !!! ce qui est vraiment super car ca nous enleve une certaine restriction et etend d'avantage nos horizons de performances , de fusions de regles et de matching d'algorihmes ..etc !!
car on ne se pose plus en plus de bloquer , de savoir quel regles a ete a l'origine du blocage (notre module s'en fiche ,du moins , a ce stade de preuve de concept)
car on pourrait bibien presenter un module qui ne sait peut etre pas quelle reglew a match et ce serait son defaut peut etre mais avec pleins d'avantages comme la vitesse et de meilleurs algorithmes
maintenant la méthode nouvelle va consister en quelque sorte à une optimisation des règles une optimisation des règles parce que la méthode optimisation à nous seront au niveau des règles tu as vu il ya d'abord une optimisation au niveau de l'architecturé un peu comment cette histoire d'agréger tout mettre un truc qui fit que tout là au début et il ya une histoire d'optimisation des des règles qui est hyper importante. C'est l'approche que je veux maintenant, sur laquelle je vais travailler. Et elle est très importante. Maintenant, parlons de cette approche. Ce qui va se passer, c'est quoi? Ce qui va se passer, c'est que lorsque l'on va récupérer les règles, prenons pas, je vais te prendre l'exemple le plus simple pour qu'on puisse bien le comprendre. je prends l'exemple du cas où on a plusieurs IPS dans la chaîne. Notre module récupère les règles de plusieurs IPS. Admettons par exemple qu'il est capable de lancer des workers de filtrage avec les ensembles de règles des différents IPS. Il pourrait. Ça, ce serait la méthode parallèle. Non, on ne le fera pas. Ce qu'il va faire, c'est qu'il va les mettre toutes. Il va les combiner toutes. va combiner toutes ces règles-là, tu me diras que ça fera juste un gros fichier de règles, un gros ensemble de règles. Mais c'est là qu'intervient la subtilité. C'est qu'on va optimiser ce fichier au maximum. C'est-à-dire qu'on va faire plein d'opérations dessus. C qu va commencer par supprimer tous les doublons Imaginons par exemple qu ait mis tout l de r communautaires dans un filtreur on a encore mis le m ensemble dans notre filtreur avant de rajouter des r en plus là par exemple on pourrait déjà supprimer les doublons ça c'est déjà quelque chose d'intéressant c'est déjà plus intéressant qu'une simple concaténation des règles de tout le monde c'est déjà plus intéressant la suppression de doublons mais on ne s'arrête pas là parce qu'on qu'on peut aussi faire de la fusion de règles. Imaginons, là je m'intéresse, et je préfère le dire tout de suite, on ne s'intéresse pour notre preuve de concept, pour le moment, qu'aux règles qui consistent à faire matcher des patterns, ou des règles qui sont, c'est un peu comme des règles de hippie port, et les règles qui consistent à faire matcher des patterns. Par pattern, je veux dire qu'on cherche en fait, des motifs à l'intérieur des paquets, des requêtes, etc. Je ne veux pas des règles comme ça. Je veux dire par là que j'exclus les règles qui prennent en compte le temps, les règles qui prennent en compte, par exemple, le nombre de paquets envoyés. Je veux dire, par exemple, je ne veux pas les règles qui permettent de détecter les attaques des doses, par exemple, parce que pour ce genre de règles-là, il faut avoir certaines statistiques de flux, chose que notre preuve de concept n'aura pas. ces histoires de statistiques de flux, etc. Non, pour l'instant, on va travailler avec des règles qui sont entre guillemets simples, même si en réalité, les règles de pattern matching ne sont pas vraiment toutes simples en réalité, pas vraiment simples. Donc voilà, on prend ça et on fusionne aussi certaines règles. Imaginons par exemple qu'on ait une règle qui detecte le mot virus dans une requete peut d requ et une autre r qui d le mot virus3 l d requ une autre règle qui détecte le mot virus12345 à l'intérieur d'une requête, une autre règle qui détecte le mot virus1234567. Tu vois que d'une certaine façon, on pourrait combiner ces trois règles-là en une seule règle qui, en fait, détecte le pattern virus suivi de, je ne sais pas, peut-être étoile ou suivi de 1 ou 2 ou 3 ou 4 (virus* ou virus1|2|3..etc). Donc, je vais dire, on peut combiner ces trois règles-là, ce qui serait une seule règle, en réalité, à ce moment-là. Voilà. Combiner à ça maintenant le fait qu'on se serve des dernières technologies et des meilleurs algorithmes de matching, comme HyperScan, AOCORASIC, etc., etc. on peut même regarder ces codes là dans le dans le dépôt de ce mode ou chercher carrément c'est les impératations de ces algorithmes nation tn est vraiment travailler avec et comme j'ai dit comment va-t-on aura fait le tir en c'est ou en c++ vraiment hyper scan on peut utiliser la lettre telly on peut utiliser vraiment les meilleurs librairies externe si on peut se le permettre Si on peut même faire des trucs qui sont exécutés dans le noyau grâce à des outils comme eBPF qui permettent d'accélérer les choses, vraiment on le fait, on fait les meilleures choses. Voilà en quoi on va consulter vraiment notre travail de recherche. Et nous esp donc qu optimisation des r c apr agr des r parce qu y aura d une de traduction mais qui ne nous int pas trop parce que je pense que pour notre propre concept on va partir avec un fichier de r d On esp que apr optimisation des r la vitesse qu va observer sera plus grande parce que d nous ce travail est d très intéressant dans la mesure il va permettre déjà de de pallier à ce problème de d'empilage de filtre réseau là mais on voudrait aussi que notre module soit rapide c'est à dire qu'il doit maintenant aussi être rapide donc pas juste une concat on doit prouver que c'est pas juste une concaténation des règles c'est danser concaténation mais vraiment optimisé au maximum qu'il faut maximum agrégé au maximum et tout et tout et tout donc tout vraiment très bien fait de telle sorte que même après carrément l'opération d'optimisation de règles on peut quitter de 10 mille règles seulement à seulement 300 règles à seulement 300 règles par exemple donc vraiment il ya vraiment un très intéressant travail d'optimisation du fichier de règles à fait et je suis très conscient qu'en fait cette optimisation des fichiers de règles lors dépend très fortement des règles qu'on a du fichier de règles qu'on a est un des défauts mais il faut l'accepter il dépend très fortement c'est à dire que pour deux fichiers de règles donnés l'optimisation sera forcément différente forcément ou le moins le niveau du gain qu'on pourra avoir sera forcément différent et après il faudra tester il faudra tester notre truc avec un fichier de règles non optimisé et tester avec un fichier de règles optimisé voilà donc fichier de gain c'est à dire optimisé on mesure le gain et on voit si vraiment on obtient un gain significatif allez, on fait notre papier. Et ce ne sera pas fait en fait pour cette preuve de concept. Je pense qu'on s'est bien compris.
voilà voilà comme j'ai dit que ça dépendait fortement du fichier de règles il nous faut effectivement un fichier de règles pour tester cette pour implémenter cette optimisation sans même déjà cette optimisation là sera le gros du travail vraiment un très gros travail avec des algos qu'il faudra vraiment réfléchir à des algos exceptionnels et il y aura ce travail d'optimisation là qui sera gros et un autre travail aussi qui est vraiment très intéressant c'est le travail de filtrage même, c'est-à-dire le travail de filtrage, le travail de matching de patterns, c'est-à-dire matching de signatures avec les meilleurs argots qui existent et tout, et tout, et tout. Donc il y a ça. Il y avait aussi des questions que je me posais, comme par exemple, vu notre fusion de règles qu'on va faire, je me demandais si, je sais que les systèmes de prévention d'intrusion moderne, actuelle, ne font pas cette fusion-là, Mais je veux savoir si en fait, lorsqu'ils font match, lorsqu'ils s'apprêtent à vérifier si un paquet va peut-être passer une signature, est-ce qu'ils ne font pas une fusion de tous les patterns pour vérifier ? Ou est-ce qu'ils vérifient quand même les règles séquentiellement ? C'est ce que je me demande. c'est ce que je me dis, où est-ce qu'ils vérifient les règles les unes après les autres parce que notre truc va un peu grâce à notre truc s'ils font les règles les unes après les autres notre travail sera vraiment très intéressant parce que grâce à ça ça va réduire le nombre de règles donc j'aimerais savoir si avec la façon de faire des IPS actuels r le nombre de r serait int aussi c ce qu font d lorsqu travaillent en interne voil je parlais de bonnes technologies utiliser comme hyperscan et tout et tout donc voilà un peu l'objectif du projet c'est tellement à expliquer mais ça peut être un projet de recherche un projet complexe donc pour notre preuve de concept comme j'ai dit le gros du travail cette optimisation de fichiers de règles à l'autre gros du travail parce que à ce moment là il faudra vraiment tester c'est à dire vraiment fait du filtrage de deux paquets du filtrage de vue c'est à dire que vérifier les patins à l'intérieur dont il faudra fait du réassemblage tcp rapide et intéressant si même s'il ya des librairies externe qui le font ou les meilleurs algos qui le font, il faut que ce soit rapide, qu'on check les choses vraiment de façon intéressante. Utiliser l'autre bout du travail, le matching de règles, le faire avec les meilleurs algorithmes et les meilleures technologies. Effectivement, même s'il faut qu'on calque sur le cours de d'autres, ça reste toujours très intéressant. Donc, on voit un peu en quoi cela va consister. le matching de règles, le réassemblage, l'optimisation de fichiers de règles, etc. Même si notre mesure va surtout porter sur le débit et la bande passante lorsque l'on a des règles optimisées et lorsqu'on a des règles pas optimisées. Donc, voilà le travail.
pour les regles que j'ai envie d'utiliser pour la prevuce de concept et mes tests on va partir sur des regles simples (ip , ports , verfifications de patterns ..etc)
meme si j'ai conscience que les regles ip/ports sont un peu differents des regles patterns patching (deja elles n'ont pas besoin de reassemblage , et en genreal on les gere plus vite) ce sont donc les premeireres a verfier et avec leur systeme de verification ultra rapide apart , avant d'arriver a notre automate ..etc , et je me dis meme que avant le pattern matching il y a une partie ip port a verifier mais souvent ce seojt des regles "any" donc pas trop de quoi s'en faire a ce niveau.
cela veut aussi dire que la preparation des "SETS" de regles est vraiment une etape ultra indispensable et importante , surtout que le DSL qu'on va utiliser est asseaz proche de celui des "IPS" , meme si le notre (ou celui des IPS carrement) peut permettre une decompositon de regles en vecteurs d'attributs par exemple , et reorganisercela bien en arbre.
si il faut un temps de preptraitement , alors soit , il peut etre aussi long qu'il veut. la memire aussi on la considere comme etant illimitee , donc on ne se restrindra pas dessus. on se concentre surtout sur cette histoire fusuion(optimisation) des regles (meme si c'est en memoire RAM) de de filtrage , reassemblage rapide ..etc. et que ca marche !!
Aussi je me rends compte à l'instant de quelque chose, du fait que pour pouvoir élaborer notre preuve de concept il nous faudra un ensemble de règles. Donc un set de règles, un ensemble de règles bien connus que l'on peut retrouver sur internet. J'ai dit qu'on allait partir sur des règles de système de prévention d'intrusion, ce serait bien, et des règles de préférence. On va prendre des règles simples. Par règles simples je veux dire des règles qui ne prennent pas en compte et le nombre de paquets c'est à dire les statiques qui ne prennent pas en compte les statistiques de flux je pense que tu m'as bien compris lorsque je parlais de statistiques de flux donc c'est déjà d'ensemble de règles déjà dont on a besoin ou il n'y en a pas qui sont directement comme ça on prend et je veux que les 7 de règles doivent être grands on prend les 7 de règles existantes bien connues et on retire juste ces règles qui ne nous intéressent pas pour le moment là les règles qui font référence aux statistiques de flux et ira donc on les on les enl ou on retire simplement des f de flux dedans m si je ne pense pas que retirer les stades de flux en dehors de ces r sont int Je pense qu'on devrait juste retirer les règles carrément qui font intervenir les stades de flux. Donc, on pourrait travailler comme ça. On prend un set de règles. Un set de règles bien connu, bien référencé, parce que dans le papier, il faudrait qu'on dise quel est le set de règles qu'on a utilisé et quelles sont les opérations qu'on a fait dessus. c'est-à-dire les règles qu'on a juste enlevées dedans, par exemple. Et de préférence, ce sera aussi bien que j'ai un script qui me permette de retirer les règles que je ne juge pas intéressantes dedans. Parce que, par exemple, s'il faut que je me mette à retirer les règles qui ne sont pas le style de règles que je cherche pour ma preuve de concept, ça risque d'être assez long si je le fais à la main. parce que bon, ce serait quand même très difficile si un ensemble de règles, de 6000 règles par exemple, parce que souvent dans les ensembles, il y a des milliers de règles, donc ce serait extrêmement compliqué. Donc voilà un peu le délit, le principe.


# 1. Introduction and Scientific Context

### The "Security Stacking" Problem
In modern infrastructures, packets traverse a sequential chain of security devices:
`L3/L4 Firewall` $\rightarrow$ `IDS/IPS (Snort/Suricata)` $\rightarrow$ `WAF (ModSecurity)`

Each device adds:
*   Processing latency (parsing, matching).
*   Memory copies (Zero-Copy impossible on a heterogeneous chain).
*   Redundant CPU consumption (checking 3 times if the IP is not blacklisted).

**Consequence:** A drastic drop in useful throughput (up to -80% observed) and increased latency (Jitter).

### The "Early Rejection" Concept
The idea is to move the blocking decision (`DROP`) as far upstream as possible.
If a packet is destined to be rejected by the IPS (step 2) because of its content, why waste CPU cycles in the Firewall (step 1)?

Our project aims to **mathematically unify** all these rules into a single decision graph, placed at the head of the chain.

---

## 2. Objectives and Research Hypothesis

### Hypothesis
It is possible to compile a heterogeneous set of rules (Firewall + IPS) into a **unified data structure** (Trees + Automata) that is:
1.  More compact (fewer rules to check).
2.  Faster (logarithmic complexity $O(\log N)$ instead of linear $O(N)$).
3.  Strictly equivalent in terms of security (no induced false negatives).

### Why This Isn't Just "Better Snort"?
Engines like Snort optimize *matching* (finding a pattern), but not the *logical structure* of the rules.
*   **Snort:** Reads 10 similar rules as 10 distinct entities.
*   **Our Optimizer:** Merges these 10 rules into 1 complex mathematical entity.

**Consequence:** Our optimized rules **ARE NO LONGER** compatible with Snort. They are intended for a dedicated C++ engine (`FoxEngine`) capable of understanding these merged structures.

---

## 3. Global System Architecture

The project is divided into two distinct components to separate intelligence (slow) from execution (fast).

### A. The Preprocessor (Python) - *This repository*
*   **Role:** Rule compiler ("Offline").
*   **Input:** Standard text files (`snort3-community.rules`).
*   **Processing:** Semantic analysis, Set algebra, Graph theory.
*   **Output:** Optimized binary artifacts and scripts.
*   **Constraint:** No time limit (can take 10 min to compile 10k rules).

### B. The Runtime Engine (C++) - *Future repository*
*   **Role:** Real-time execution ("Online").
*   **Input:** Artifacts generated by Python.
*   **Technologies:** `NFQUEUE` (interception), `Hyperscan` (Intel Regex), `mmap` (binary loading).
*   **Constraint:** Absolute performance (Zero-Copy).
